
services:
  jupyter_spark:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: jupyter_spark
    user: jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./:/workspace 
    restart: unless-stopped

  postgres:
    image: postgres:15
    container_name: dv_postgres
    env_file:
      - .env
    environment:
      POSTGRES_DB: gdelt
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./workspace/data/silver:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__DAGS_FOLDER=/workspace/airflow/dags
      - AIRFLOW__CORE__PARALLELISM=4
      - AIRFLOW__CORE__DAG_CONCURRENCY=4
      - AIRFLOW__WEBSERVER__WORKERS=2
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP=5
      - AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY=32
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://gdelt_user:gdelt_pass@postgres:5432/airflow_meta
    volumes:
      - ./:/workspace
    ports:
      - "8080:8080"
  # Startup handled by Dockerfile CMD (webserver + scheduler)
    restart: unless-stopped

  dbt_postgres:
    image: ghcr.io/dbt-labs/dbt-postgres:1.9.latest
    container_name: dbt_postgres
    volumes:
      - ./:/workspace
    depends_on:
      - postgres
    working_dir: /workspace
    entrypoint: ["tail", "-f", "/dev/null"]
    restart: unless-stopped

